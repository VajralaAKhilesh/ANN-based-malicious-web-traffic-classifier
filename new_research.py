# -*- coding: utf-8 -*-
"""NEW Research.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1m9EB2arNkxyRkaKnhGw9iFt4hHDah2Ha
"""

import numpy as np
import pandas as pd

df=pd.read_csv("/content/merged_malicious_beging_dataset.csv")

df.info()

df.head(10)

from sklearn.impute import SimpleImputer
from sklearn.preprocessing import KBinsDiscretizer
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler

pip install explainerdashboard

pip install orange3

pip install bottleneck

pip install --upgrade scikit-learn

from pathlib import Path
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
import pandas as pd
import numpy as np
from explainerdashboard import ClassifierExplainer, ExplainerDashboard


pkl_dir = Path.cwd() / "pkls"
data_dir= Path.cwd() / "data"
df=pd.read_csv(data_dir /'/content/merged_malicious_beging_dataset.csv')
# df.head()
y=df['DoH']
X=df.drop(columns=['DoH'])



# classifier
from sklearn.model_selection import train_test_split

# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42, stratify=y)

X_train, X_test,y_train, y_test=train_test_split(X,y,test_size = 0.2, stratify=y, random_state=42)
# classifier
# model = RandomForestClassifier(n_estimators=10, random_state=42,max_depth=5,class_weight='balanced')
model = RandomForestClassifier(n_estimators=10, random_state=42,max_depth=5)

model.fit(X_train, y_train)

# Take the first 10 rows of X_test as a new sample
new_sample = X_test.iloc[0:10, :]

# Make predictions on the new test sample
predictions = model.predict(new_sample)

# Print the new sample and predictions
print(new_sample)
print(f"\nPredictions for the new sample:")
for i, pred in enumerate(predictions):
    print(f"Sample {i + 1}: {pred}")

from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Make predictions on the entire test set
y_pred = model.predict(X_test)

# Create a confusion matrix
cm = confusion_matrix(y_test, y_pred)

# Plot the confusion matrix using seaborn
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=['Non-DoH', 'Benign-DoH', 'Malicious-DoH'], yticklabels=['Non-DoH', 'Benign-DoH', 'Malicious-DoH'])
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.title('Confusion Matrix')
plt.show()

"""F1 score"""

from sklearn.metrics import f1_score

# Make predictions on the test set
y_pred = model.predict(X_test)

# Calculate F1 score
f1 = f1_score(y_test, y_pred)

print(f"F1 Score: {f1}")

"""RECALL

"""

from sklearn.metrics import recall_score

# Make predictions on the entire test set
y_pred = model.predict(X_test)

# Calculate recall for each class
recall_per_class = recall_score(y_test, y_pred, average=None)

# Print recall for each class
for i, recall in enumerate(recall_per_class):
    print(f"Recall for Class {i}: {recall:.4f}")

# Calculate and print macro-average recall
macro_avg_recall = recall_score(y_test, y_pred, average='macro')
print(f"\nMacro-average Recall: {macro_avg_recall:.4f}")

"""ACCURACY"""

from sklearn.metrics import accuracy_score

# Make predictions on the entire test set
y_pred = model.predict(X_test)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)

# Print the accuracy
print(f"Accuracy: {accuracy:.4f}")

df['DoH'].value_counts()

"""ANN-Model"""

from keras.models import Sequential
from keras.layers import Dense
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import confusion_matrix, classification_report

# Standardize the data
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, stratify=y, random_state=42)

# Build the ANN model
model = Sequential()
model.add(Dense(64, input_dim=X.shape[1], activation='relu'))
model.add(Dense(32, activation='relu'))
model.add(Dense(3, activation='softmax'))  # Assuming 3 classes

# Compile the model
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Train the model
model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)

# Evaluate the model on the test set
loss, accuracy = model.evaluate(X_test, y_test)
print(f"\nTest Loss: {loss:.4f}")
print(f"Test Accuracy: {accuracy:.4f}")

# Make predictions on the test set
y_pred = model.predict(X_test)
y_pred = np.argmax(y_pred, axis=1)

# Print classification report
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

# Print confusion matrix
cm = confusion_matrix(y_test, y_pred)
print("\nConfusion Matrix:")
print(cm)

"""F1 Score"""

f1 = f1_score(y_test, y_pred, average='weighted')  # Choose the appropriate averaging strategy

print(f"\nWeighted F1 Score: {f1:.4f}")

"""RECALL"""

recall_per_class = recall_score(y_test, y_pred, average=None)

# Print recall for each class
for i, recall in enumerate(recall_per_class):
    print(f"Recall for Class {i}: {recall:.4f}")

# Calculate and print macro-average recall
macro_avg_recall = recall_score(y_test, y_pred, average='macro')
print(f"\nMacro-average Recall: {macro_avg_recall:.4f}")

from keras.models import Sequential
from keras.layers import Dense, LSTM
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import confusion_matrix, classification_report

# Standardize the data
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, stratify=y, random_state=42)

# Reshape the input data for LSTM
X_train_lstm = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))
X_test_lstm = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))

# Build the LSTM model
model = Sequential()
model.add(LSTM(64, input_shape=(1, X_train.shape[1]), activation='relu'))
model.add(Dense(32, activation='relu'))
model.add(Dense(3, activation='softmax'))  # Assuming 3 classes

# Compile the model
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Train the model
model.fit(X_train_lstm, y_train, epochs=20, batch_size=32, validation_split=0.3)

# Evaluate the model on the test set
loss, accuracy = model.evaluate(X_test_lstm, y_test)
print(f"\nTest Loss: {loss:.4f}")
print(f"Test Accuracy: {accuracy:.4f}")

# Make predictions on the test set
y_pred_probs = model.predict(X_test_lstm)
y_pred = np.argmax(y_pred, axis=0)

from scipy.sparse import coo_matrix
from sklearn.metrics import classification_report

# Assuming y_pred is a 1D array of predicted class labels
y_pred = np.argmax(y_pred, axis=0)

# Print classification report
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

# Print classification report
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

# Print confusion matrix
cm = confusion_matrix(y_test, y_pred)
print("\nConfusion Matrix:")
print(cm)

# Calculate and print F1 score
f1 = f1_score(y_test, y_pred, average='weighted')
print(f"\nWeighted F1 Score: {f1:.4f}")

"""CNN Model"""

import tensorflow as tf
from tensorflow.keras import layers, models
from sklearn.model_selection import train_test_split

# Assuming X and y are your data and labels
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# Assuming input shape is (height, width, channels) of your data
input_shape = (64, 64, 3)  # Adjust these dimensions based on your data

# Define the CNN model
model = models.Sequential()

# Convolutional layers
model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (3, 3), activation='relu'))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(128, (3, 3), activation='relu'))
model.add(layers.MaxPooling2D((2, 2)))

# Flatten layer
model.add(layers.Flatten())

# Fully connected layers
model.add(layers.Dense(128, activation='relu'))
model.add(layers.Dropout(0.5))  # Optional dropout layer for regularization
model.add(layers.Dense(1, activation='sigmoid'))  # Binary classification

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Display the model summary
model.summary()

# Train the model with validation data
model.fit(X_train, y_train, epochs=10, validation_data=(X_val, y_val))

import tensorflow as tf
from tensorflow.keras import layers, models
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Assuming X and y are your data and labels
# Assuming X contains your input features and y contains your labels
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)


# CNN Model
cnn_model = models.Sequential()
cnn_model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)))
cnn_model.add(layers.MaxPooling2D((2, 2)))
cnn_model.add(layers.Conv2D(64, (3, 3), activation='relu'))
cnn_model.add(layers.MaxPooling2D((2, 2)))
cnn_model.add(layers.Flatten())
cnn_model.add(layers.Dense(128, activation='relu'))
cnn_model.add(layers.Dense(1, activation='sigmoid'))
cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# ANN Model
ann_model = models.Sequential()
ann_model.add(layers.Dense(128, activation='relu', input_shape=(X_train.shape[1],)))
ann_model.add(layers.Dropout(0.5))
ann_model.add(layers.Dense(1, activation='sigmoid'))
ann_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the models
cnn_model.fit(X_train, y_train, epochs=5, validation_data=(X_val, y_val), verbose=0)
ann_model.fit(X_train, y_train, epochs=5, validation_data=(X_val, y_val), verbose=0)

# Make predictions
cnn_predictions = cnn_model.predict(X_val)
ann_predictions = ann_model.predict(X_val)

# Ensemble Predictions (Simple Averaging)
ensemble_predictions = (cnn_predictions + ann_predictions) / 2

# Convert to binary predictions
ensemble_binary_predictions = (ensemble_predictions > 0.5).astype(int)

# Evaluate ensemble model
ensemble_accuracy = accuracy_score(y_val, ensemble_binary_predictions)
print(f"Ensemble Model Accuracy: {ensemble_accuracy}")

